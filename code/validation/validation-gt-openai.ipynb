{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of the gender representation bias quantification method\n",
    "\n",
    "Validate the LLM-based gender representation bias quantification method on an annotated dataset.\n",
    "\n",
    "Using OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "from openai import OpenAI\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"paste-your-api-key-here\"\n",
    "model = \"gpt-4-turbo\"\n",
    "analysis_file_path = \"../../data/validation/\"\n",
    "analysis_file_id = f\"{model}.01\"\n",
    "gt_file = \"../../data/validation/gt-es.txt\"\n",
    "examples_pathname = \"../../data/dataset-analysis/examples-es.txt\"\n",
    "num_sentences = 100 # number of sentences to be analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the ground truth file, process its content, and return a DataFrame\n",
    "def df_func(gt_file):\n",
    "    \n",
    "    data = []\n",
    "\n",
    "    with open(gt_file, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        blocks = content.split('\\n\\n')\n",
    "        for block in blocks:\n",
    "            sentence, words = block.strip().split(' \"')\n",
    "            sentence = sentence.strip('\"')\n",
    "            words = words.strip('\"')\n",
    "            data.append((sentence, words))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['Ejemplo', 'Respuesta'])\n",
    "    return df\n",
    "\n",
    "df = df_func(gt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the examples\n",
    "with open(examples_pathname, 'r', encoding='utf-8') as file:\n",
    "    examples = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sentence from DataFrame\n",
    "def extract_sentence(df, sentence_id):\n",
    "\n",
    "    sentence_str = str(df['Ejemplo'][sentence_id])\n",
    "    return sentence_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the model response to the DataFrame\n",
    "def final_df_func(df, response, i):\n",
    "\n",
    "    final_df = df.copy()\n",
    "    final_df.loc[i, 'Respuesta modelo'] = response\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the model response into a list of words\n",
    "def response_trans_func(final_df, i):\n",
    "\n",
    "    words = re.split(\" - |, |\\n\", final_df[\"Respuesta modelo\"][i])\n",
    "    word_list = [words[i:i+3] for i in range(0, len(words), 3)]\n",
    "    return word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the ground truth file, process its content, and return a DataFrame\n",
    "def df_trans_func(gt_file):\n",
    "\n",
    "    data = []\n",
    "\n",
    "    with open(gt_file, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        blocks = content.split('\\n\\n')\n",
    "\n",
    "        for block in blocks:\n",
    "            sentence, words = block.strip().split(' \"')\n",
    "            sentence = sentence.strip('\"')\n",
    "            words = words.strip('\"')\n",
    "            words = words.upper()\n",
    "            words = re.split(\" - |, |\\n\", words)\n",
    "            word_list = [words[i:i+3] for i in range(0, len(words), 3)]\n",
    "            data.append((sentence, word_list))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['Ejemplo', 'Respuesta'])\n",
    "    return df\n",
    "\n",
    "df_trans_func(gt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the model response with the ground truth\n",
    "def analysis(index, word_list, gt):\n",
    "    \n",
    "    identified = [] # Words correctly identified and correctly classified in both attributes (n_c)\n",
    "    not_identified = [] # Words missed (not identified) by the model (n_m)\n",
    "    wrongly_analyzed = [] # Words correctly identified but incorrectly classified in at least one attribute (n_i)\n",
    "    wrongly_identified = word_list.copy() # Extra words that do not appear in the ground truth but were returned by the model (n_e)\n",
    "\n",
    "    for k in range(len(gt[index])):\n",
    "        if gt[index][k] in wrongly_identified:\n",
    "            identified.append(gt[index][k])\n",
    "            position = wrongly_identified.index(gt[index][k])\n",
    "            del wrongly_identified[position]\n",
    "        elif gt[index][k][0] not in [sublista[0] for sublista in wrongly_identified]:\n",
    "            not_identified.append(gt[index][k])\n",
    "        elif gt[index][k] not in wrongly_identified and gt[index][k][0] in [sublista[0] for sublista in wrongly_identified]:\n",
    "            wrongly_analyzed.append(gt[index][k])\n",
    "            for sublist in wrongly_identified:\n",
    "                if gt[index][k][0] == sublist[0]:\n",
    "                    wrongly_identified.remove(sublist)\n",
    "\n",
    "    return identified, not_identified, wrongly_analyzed, wrongly_identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis wrapper\n",
    "def analysis_wrapper(final_df, i):\n",
    "\n",
    "    word_list = response_trans_func(final_df, i)\n",
    "    gt = df_trans_func(gt_file)[\"Respuesta\"]\n",
    "    identified, not_identified, wrongly_analyzed, wrongly_identified  = analysis(i, word_list, gt)\n",
    "    \n",
    "    return f'{i+1} | {extract_sentence(df, i)} | {df_trans_func(gt_file)[\"Respuesta\"][i]} | {word_list} | {len(identified)} | {len(not_identified)} | {len(wrongly_analyzed)} | {len(wrongly_identified)} | {identified} | {not_identified} | {wrongly_analyzed} | {wrongly_identified}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI API\n",
    "client = OpenAI()\n",
    "\n",
    "# Analyze the sentences with the model\n",
    "with open(f\"aux-model-output-{analysis_file_id}.csv\", mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for i in tqdm(range(num_sentences), desc=\"Processing sentences\"):\n",
    "\n",
    "        sentence = extract_sentence(df, i) # Frase a analizar\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"{examples}\\\\nFrase: {sentence}\\\\nInstrucciones: Identifica todos los sustantivos y pronombres en la frase proporcionada. Para cada uno, determina si se refiere a un ser humano (S) o no (N), y especifica su género gramatical: masculino (M) o femenino (F). Excluye los apellidos. Sigue el formato de los ejemplos proporcionados sin añadir texto adicional.\"}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        response = response.choices[0].message.content\n",
    "        writer.writerow([f'{i+1} | {sentence} | {df[\"Respuesta\"][i]} | {re.sub(r'\\s+\\n', '\\n', response.strip().upper())}'])\n",
    "        file.flush()\n",
    "\n",
    "response_model = f\"aux-model-output-{analysis_file_id}.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the model response\n",
    "def df_model_process(response_model):\n",
    "    df_model = pd.read_csv(response_model, header=None)\n",
    "    df_model = df_model[0].str.split('|', expand=True)\n",
    "    df_model.columns = ['ID', 'Frase', 'GT', 'Respuesta']\n",
    "\n",
    "    # Remove whitespaces\n",
    "    df_model = df_model.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    return df_model\n",
    "\n",
    "df_model = df_model_process(response_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "def evaluation(num_sentences):\n",
    "\n",
    "    with open(f\"aux-validation-{analysis_file_id}.csv\", mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for i in range(num_sentences):\n",
    "            response = df_model[\"Respuesta\"][i]\n",
    "            final_df = final_df_func(df, response, i)\n",
    "            final = analysis_wrapper(final_df, i)\n",
    "            writer.writerow([final])\n",
    "\n",
    "    df_complete = pd.read_csv(f'aux-validation-{analysis_file_id}.csv', header=None)\n",
    "\n",
    "    # Divide every row in 4 parts using the '|' separator\n",
    "    df_complete = df_complete[0].str.split('|', expand=True)\n",
    "\n",
    "    df_complete.columns = ['ID', 'Frase', 'GT', 'Respuesta', 'N identificadas', 'N no identificadas', 'N mal analizadas', 'N mal identificadas', 'Identificadas', 'No identificadas', 'Mal analizadas', 'Mal identificadas']\n",
    "\n",
    "    # Remove whitespaces\n",
    "    df_complete = df_complete.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "    return df_complete\n",
    "\n",
    "evaluation = evaluation(num_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove auxiliary files\n",
    "os.remove(f\"aux-model-output-{analysis_file_id}.csv\")\n",
    "os.remove(f\"aux-validation-{analysis_file_id}.csv\")\n",
    "\n",
    "# Write output to Excel file\n",
    "with pd.ExcelWriter(os.path.join(analysis_file_path, f'validation-{analysis_file_id}.xlsx')) as writer:\n",
    "    evaluation.to_excel(writer, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
